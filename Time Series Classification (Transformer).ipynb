{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d2718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54eb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bcd84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bbfb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5531628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardisation of Time and Amount variables\n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "df['Time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd62cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Class'], axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8e21bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.996583</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.996583</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.996541</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>1.641931</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>-0.350151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>1.641952</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>-0.254117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.641974</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>-0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>1.641974</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>-0.313249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>1.642058</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.514355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0      -1.996583  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1      -1.996583   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2      -1.996562  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3      -1.996562  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4      -1.996541  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  1.641931 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  1.641952  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  1.641974   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  1.641974  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  1.642058  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V20       V21  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ...  0.251412 -0.018307   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.524980  0.247998   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  1.475829  0.213454   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.059616  0.214205   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.001396  0.232045   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.127434  0.265245   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "          Amount  \n",
       "0       0.244964  \n",
       "1      -0.342475  \n",
       "2       1.160686  \n",
       "3       0.140534  \n",
       "4      -0.073403  \n",
       "...          ...  \n",
       "284802 -0.350151  \n",
       "284803 -0.254117  \n",
       "284804 -0.081839  \n",
       "284805 -0.313249  \n",
       "284806  0.514355  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0504c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93969d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "x_test = x_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d0428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train).reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = np.array(x_test).reshape(x_test.shape[0], x_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf12555",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26702e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ca14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    return tn / (tn + fp + K.epsilon())\n",
    "\n",
    "\n",
    "def negative_predictive_value(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "    return tn / (tn + fn + K.epsilon())\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    num = (1 + beta ** 2) * (p * r)\n",
    "    den = (beta ** 2 * p + r + K.epsilon())\n",
    "    return K.mean(num / den)\n",
    "\n",
    "\n",
    "def matthews_correlation_coefficient(y_true, y_pred):\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / K.sqrt(den + K.epsilon())\n",
    "\n",
    "\n",
    "def equal_error_rate(y_true, y_pred):\n",
    "    n_imp = tf.count_nonzero(tf.equal(y_true, 0), dtype=tf.float32) + tf.constant(K.epsilon())\n",
    "    n_gen = tf.count_nonzero(tf.equal(y_true, 1), dtype=tf.float32) + tf.constant(K.epsilon())\n",
    "\n",
    "    scores_imp = tf.boolean_mask(y_pred, tf.equal(y_true, 0))\n",
    "    scores_gen = tf.boolean_mask(y_pred, tf.equal(y_true, 1))\n",
    "\n",
    "    loop_vars = (tf.constant(0.0), tf.constant(1.0), tf.constant(0.0))\n",
    "    cond = lambda t, fpr, fnr: tf.greater_equal(fpr, fnr)\n",
    "    body = lambda t, fpr, fnr: (\n",
    "        t + 0.001,\n",
    "        tf.divide(tf.count_nonzero(tf.greater_equal(scores_imp, t), dtype=tf.float32), n_imp),\n",
    "        tf.divide(tf.count_nonzero(tf.less(scores_gen, t), dtype=tf.float32), n_gen)\n",
    "    )\n",
    "    t, fpr, fnr = tf.while_loop(cond, body, loop_vars, back_prop=False)\n",
    "    eer = (fpr + fnr) / 2\n",
    "\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b3f1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4228503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "#     outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb570f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 30, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 30, 1)       2           ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 30, 1)       7169        ['layer_normalization_16[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 30, 1)        0           ['multi_head_attention_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 30, 1)       0           ['dropout_18[0][0]',             \n",
      " ambda)                                                           'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 30, 1)       2           ['tf.__operators__.add_16[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 30, 4)        8           ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 30, 4)        0           ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 30, 1)        5           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 30, 1)       0           ['conv1d_17[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 30, 1)       2           ['tf.__operators__.add_17[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 30, 1)       7169        ['layer_normalization_18[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 30, 1)        0           ['multi_head_attention_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 30, 1)       0           ['dropout_20[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 30, 1)       2           ['tf.__operators__.add_18[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 30, 4)        8           ['layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 30, 4)        0           ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 30, 1)        5           ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 30, 1)       0           ['conv1d_19[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 30, 1)       2           ['tf.__operators__.add_19[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 30, 1)       7169        ['layer_normalization_20[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 30, 1)        0           ['multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 30, 1)       0           ['dropout_22[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 30, 1)       2           ['tf.__operators__.add_20[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 30, 4)        8           ['layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 30, 4)        0           ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 30, 1)        5           ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 30, 1)       0           ['conv1d_21[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 30, 1)       2           ['tf.__operators__.add_21[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 30, 1)       7169        ['layer_normalization_22[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 30, 1)        0           ['multi_head_attention_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 30, 1)       0           ['dropout_24[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 30, 1)       2           ['tf.__operators__.add_22[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 30, 4)        8           ['layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 30, 4)        0           ['conv1d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 30, 1)        5           ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 30, 1)       0           ['conv1d_23[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 30)          0           ['tf.__operators__.add_23[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          3968        ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            129         ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,841\n",
      "Trainable params: 32,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "1781/1781 [==============================] - 1654s 921ms/step - loss: 0.0258 - matthews_correlation_coefficient: 0.0196 - val_loss: 0.0068 - val_matthews_correlation_coefficient: 0.0458\n",
      "Epoch 2/20\n",
      "1781/1781 [==============================] - 1578s 886ms/step - loss: 0.0069 - matthews_correlation_coefficient: 0.0394 - val_loss: 0.0049 - val_matthews_correlation_coefficient: 0.0594\n",
      "Epoch 3/20\n",
      "1781/1781 [==============================] - 1665s 935ms/step - loss: 0.0051 - matthews_correlation_coefficient: 0.0488 - val_loss: 0.0045 - val_matthews_correlation_coefficient: 0.0589\n",
      "Epoch 4/20\n",
      "1781/1781 [==============================] - 1091s 613ms/step - loss: 0.0044 - matthews_correlation_coefficient: 0.0531 - val_loss: 0.0039 - val_matthews_correlation_coefficient: 0.0737\n",
      "Epoch 5/20\n",
      "1781/1781 [==============================] - 1386s 779ms/step - loss: 0.0039 - matthews_correlation_coefficient: 0.0533 - val_loss: 0.0039 - val_matthews_correlation_coefficient: 0.0806\n",
      "Epoch 6/20\n",
      "1781/1781 [==============================] - 1830s 1s/step - loss: 0.0039 - matthews_correlation_coefficient: 0.0619 - val_loss: 0.0035 - val_matthews_correlation_coefficient: 0.0828\n",
      "Epoch 7/20\n",
      "1781/1781 [==============================] - 1827s 1s/step - loss: 0.0040 - matthews_correlation_coefficient: 0.0617 - val_loss: 0.0035 - val_matthews_correlation_coefficient: 0.0800\n",
      "Epoch 8/20\n",
      "1781/1781 [==============================] - 1559s 875ms/step - loss: 0.0041 - matthews_correlation_coefficient: 0.0622 - val_loss: 0.0039 - val_matthews_correlation_coefficient: 0.0724\n",
      "Epoch 9/20\n",
      "1781/1781 [==============================] - 1243s 698ms/step - loss: 0.0037 - matthews_correlation_coefficient: 0.0612 - val_loss: 0.0033 - val_matthews_correlation_coefficient: 0.0841\n",
      "Epoch 10/20\n",
      "1781/1781 [==============================] - 1637s 920ms/step - loss: 0.0034 - matthews_correlation_coefficient: 0.0631 - val_loss: 0.0035 - val_matthews_correlation_coefficient: 0.0824\n",
      "Epoch 11/20\n",
      "1781/1781 [==============================] - 1655s 929ms/step - loss: 0.0036 - matthews_correlation_coefficient: 0.0628 - val_loss: 0.0031 - val_matthews_correlation_coefficient: 0.0863\n",
      "Epoch 12/20\n",
      "1781/1781 [==============================] - 1643s 923ms/step - loss: 0.0034 - matthews_correlation_coefficient: 0.0657 - val_loss: 0.0033 - val_matthews_correlation_coefficient: 0.0847\n",
      "Epoch 13/20\n",
      "1781/1781 [==============================] - 1194s 670ms/step - loss: 0.0033 - matthews_correlation_coefficient: 0.0632 - val_loss: 0.0031 - val_matthews_correlation_coefficient: 0.0899\n",
      "Epoch 14/20\n",
      "1781/1781 [==============================] - 1536s 863ms/step - loss: 0.0034 - matthews_correlation_coefficient: 0.0673 - val_loss: 0.0032 - val_matthews_correlation_coefficient: 0.0859\n",
      "Epoch 15/20\n",
      "1781/1781 [==============================] - 1675s 941ms/step - loss: 0.0034 - matthews_correlation_coefficient: 0.0654 - val_loss: 0.0031 - val_matthews_correlation_coefficient: 0.0899\n",
      "Epoch 16/20\n",
      "1781/1781 [==============================] - 1244s 699ms/step - loss: 0.0031 - matthews_correlation_coefficient: 0.0688 - val_loss: 0.0030 - val_matthews_correlation_coefficient: 0.0893\n",
      "Epoch 17/20\n",
      "1781/1781 [==============================] - 1252s 703ms/step - loss: 0.0032 - matthews_correlation_coefficient: 0.0685 - val_loss: 0.0033 - val_matthews_correlation_coefficient: 0.0859\n",
      "Epoch 18/20\n",
      "1781/1781 [==============================] - 1229s 690ms/step - loss: 0.0031 - matthews_correlation_coefficient: 0.0663 - val_loss: 0.0032 - val_matthews_correlation_coefficient: 0.0865\n",
      "Epoch 19/20\n",
      "1781/1781 [==============================] - 1286s 722ms/step - loss: 0.0034 - matthews_correlation_coefficient: 0.0666 - val_loss: 0.0032 - val_matthews_correlation_coefficient: 0.0861\n",
      "Epoch 20/20\n",
      "1781/1781 [==============================] - 1100s 618ms/step - loss: 0.0031 - matthews_correlation_coefficient: 0.0668 - val_loss: 0.0032 - val_matthews_correlation_coefficient: 0.0859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f946047340>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=matthews_correlation_coefficient\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.5,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5535e13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(x_test)\n",
    "y_pred = np.round(preds)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44beca4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.89      0.81      0.84        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.94      0.90      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de29e93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8456485944555958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "print(matthews_corrcoef(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "544c89fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize.draw_confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
