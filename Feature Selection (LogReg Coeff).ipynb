{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac57bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77c7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE, ADASYN \n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#from sklearn import cross_validation, linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#For Decision Tree implementation\n",
    "from scipy.stats import entropy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#For KNN implementation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For NB implementation \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "#For Logistic Regression implementation \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#For Random Forest implementation \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#For SVM implementation \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#For MLP implementation \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#For XGBoost \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#For LightGBM \n",
    "import lightgbm as lgbm\n",
    "\n",
    "#For Stacking \n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2bf93b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 14, 7\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4530c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf3fc11e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/projectgrps/IS450/IS450G10/jupyterlab-venv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Feature importance ranking using Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_importances = pd.DataFrame(data={\n",
    "    'Attribute': X_train.columns,\n",
    "    'Importance': lr.coef_[0]\n",
    "})\n",
    "lr_importances = lr_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.bar(x=lr_importances['Attribute'], height=lr_importances['Importance'], color='#087E8B')\n",
    "plt.title('Feature importances obtained from coefficients', size=20)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc023c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56ccf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.groupby('Class', group_keys=False).apply(lambda x: x.sample(100))\n",
    "stratified_kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=424)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee689f5",
   "metadata": {},
   "source": [
    "## Logistic Regression Top 2 Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674fbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Feature Selection (Top 2) \n",
    "X, y = df[['V1', 'V7']], df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e7d8f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Logistic Regression: 0.317911195864112\n",
      "Test score of Logistic Regression: 0.21134404066487072\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.41      0.11      0.17        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.70      0.55      0.59     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=424)\n",
    "\n",
    "c_values = [0.1, 10, 100]\n",
    "parameters = dict(C=c_values)\n",
    "grid_search = GridSearchCV(estimator=log_reg,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Logistic Regression: {cv_score}\\nTest score of Logistic Regression: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9da5659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Logistic Regression (SMOTE): 0.11172581801799208\n",
      "Test score of Logistic Regression (SMOTE): 0.09972922777495506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96     56880\n",
      "           1       0.02      0.73      0.03        82\n",
      "\n",
      "    accuracy                           0.93     56962\n",
      "   macro avg       0.51      0.83      0.50     56962\n",
      "weighted avg       1.00      0.93      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', LogisticRegression(random_state=424)]])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {'classifier__C':[0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=log_reg_smote_pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Logistic Regression (SMOTE): {cv_score}\\nTest score of Logistic Regression (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbcca29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Logistic Regression (ADASYN): 0.09316484930818476\n",
      "Test score of Logistic Regression (ADASYN): 0.08326452403345153\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95     56880\n",
      "           1       0.01      0.74      0.02        82\n",
      "\n",
      "    accuracy                           0.90     56962\n",
      "   macro avg       0.51      0.82      0.49     56962\n",
      "weighted avg       1.00      0.90      0.95     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', LogisticRegression(random_state=424)]])\n",
    "\n",
    "param_grid = {'classifier__C':[0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=log_reg_adasyn_pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Logistic Regression (ADASYN): {cv_score}\\nTest score of Logistic Regression (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7eaa2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Logistic Regression (NearMiss): 0.030572709548996886\n",
      "Test score of Logistic Regression (NearMiss): 0.026639133000434516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.66     56880\n",
      "           1       0.00      0.85      0.00        82\n",
      "\n",
      "    accuracy                           0.50     56962\n",
      "   macro avg       0.50      0.68      0.33     56962\n",
      "weighted avg       1.00      0.50      0.66     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', LogisticRegression(random_state=424)]])\n",
    "\n",
    "param_grid = {'classifier__C':[0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=log_reg_nearmiss_pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Logistic Regression (NearMiss): {cv_score}\\nTest score of Logistic Regression (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3252429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Naive Bayes: 0.16486919473366984\n",
      "Test score of Naive Bayes: 0.11919892522431044\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.06      0.23      0.10        82\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.53      0.61      0.55     56962\n",
      "weighted avg       1.00      0.99      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Feature Selection (Top 2) \n",
    "nb = GaussianNB()\n",
    "parameters = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "grid_search = GridSearchCV(estimator=nb,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Naive Bayes: {cv_score}\\nTest score of Naive Bayes: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "234f9d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Naive Bayes (SMOTE): 0.19017300600401027\n",
      "Test score of Naive Bayes (SMOTE): 0.14353199409293205\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.10      0.21      0.14        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.55      0.60      0.57     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                   ['classifier', GaussianNB()]])\n",
    "\n",
    "parameters = {'classifier__var_smoothing': [0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=nb_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Naive Bayes (SMOTE): {cv_score}\\nTest score of Naive Bayes (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1dd656f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Naive Bayes (ADASYN): 0.15859887807553408\n",
      "Test score of Naive Bayes (ADASYN): 0.13789154315576801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56880\n",
      "           1       0.05      0.38      0.09        82\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.53      0.68      0.54     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                   ['classifier', GaussianNB()]])\n",
    "\n",
    "parameters = {'classifier__var_smoothing': [0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=nb_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Naive Bayes (ADASYN): {cv_score}\\nTest score of Naive Bayes (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7dc2287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Naive Bayes (NearMiss): 0.17567710677146636\n",
      "Test score of Naive Bayes (NearMiss): 0.12603090471316142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.08      0.21      0.11        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.54      0.60      0.56     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                   ['classifier', GaussianNB()]])\n",
    "\n",
    "parameters = {'classifier__var_smoothing': [0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=nb_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Naive Bayes (NearMiss): {cv_score}\\nTest score of Naive Bayes (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab0dd1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Decision Tree: 0.33112753070537193\n",
      "Test score of Decision Tree: 0.2614916726503391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.43      0.16      0.23        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.72      0.58      0.62     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt= DecisionTreeClassifier(random_state=424)\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_leaf': [5, 20, 50],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Decision Tree: {cv_score}\\nTest score of Decision Tree: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b6678ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Decision Tree (SMOTE): 0.12336781768753635\n",
      "Test score of Decision Tree (SMOTE): 0.11335867084910907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98     56880\n",
      "           1       0.02      0.70      0.04        82\n",
      "\n",
      "    accuracy                           0.95     56962\n",
      "   macro avg       0.51      0.82      0.51     56962\n",
      "weighted avg       1.00      0.95      0.97     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', DecisionTreeClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__min_samples_leaf': [5, 20, 50],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Decision Tree (SMOTE): {cv_score}\\nTest score of Decision Tree (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "102d2486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Decision Tree (ADASYN): 0.09825563268920781\n",
      "Test score of Decision Tree (ADASYN): 0.09571211828826623\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96     56880\n",
      "           1       0.01      0.71      0.03        82\n",
      "\n",
      "    accuracy                           0.93     56962\n",
      "   macro avg       0.51      0.82      0.50     56962\n",
      "weighted avg       1.00      0.93      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', DecisionTreeClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__min_samples_leaf': [5, 20, 50],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Decision Tree (ADASYN): {cv_score}\\nTest score of Decision Tree (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a572003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Decision Tree (NearMiss): 0.02564887593641414\n",
      "Test score of Decision Tree (NearMiss): 0.004044583221475607\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.06      0.12     56880\n",
      "           1       0.00      0.96      0.00        82\n",
      "\n",
      "    accuracy                           0.06     56962\n",
      "   macro avg       0.50      0.51      0.06     56962\n",
      "weighted avg       1.00      0.06      0.12     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', DecisionTreeClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__min_samples_leaf': [5, 20, 50],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Decision Tree (NearMiss): {cv_score}\\nTest score of Decision Tree (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73ab9adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Random Forest: 0.3325118549599698\n",
      "Test score of Random Forest: 0.2753101969844609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.69      0.11      0.19        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.85      0.55      0.59     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=424)\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_leaf': [5, 20, 50],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Random Forest: {cv_score}\\nTest score of Random Forest: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f95f272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Random Forest (SMOTE): 0.1367797939976741\n",
      "Test score of Random Forest (SMOTE): 0.11779650856956345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98     56880\n",
      "           1       0.02      0.71      0.04        82\n",
      "\n",
      "    accuracy                           0.95     56962\n",
      "   macro avg       0.51      0.83      0.51     56962\n",
      "weighted avg       1.00      0.95      0.97     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', RandomForestClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Random Forest (SMOTE): {cv_score}\\nTest score of Random Forest (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e243f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Random Forest (ADASYN): 0.12096902636094742\n",
      "Test score of Random Forest (ADASYN): 0.10175869845991951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     56880\n",
      "           1       0.02      0.71      0.03        82\n",
      "\n",
      "    accuracy                           0.94     56962\n",
      "   macro avg       0.51      0.82      0.50     56962\n",
      "weighted avg       1.00      0.94      0.97     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', RandomForestClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Random Forest (ADASYN): {cv_score}\\nTest score of Random Forest (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac833a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Random Forest (NearMiss): 0.006938649472483911\n",
      "Test score of Random Forest (NearMiss): 0.007507015653447587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.06      0.11     56880\n",
      "           1       0.00      0.99      0.00        82\n",
      "\n",
      "    accuracy                           0.06     56962\n",
      "   macro avg       0.50      0.52      0.06     56962\n",
      "weighted avg       1.00      0.06      0.11     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', RandomForestClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Random Forest (NearMiss): {cv_score}\\nTest score of Random Forest (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e317ba20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of K Nearest Neighbours: 0.33495771912695094\n",
      "Test score of K Nearest Neighbours: 0.25287858110826344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.53      0.12      0.20        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.76      0.56      0.60     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of K Nearest Neighbours: {cv_score}\\nTest score of K Nearest Neighbours: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b2d0c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of K Nearest Neighbours (SMOTE): 0.07702844824270558\n",
      "Test score of K Nearest Neighbours (SMOTE): 0.052369057517682914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     56880\n",
      "           1       0.01      0.46      0.02        82\n",
      "\n",
      "    accuracy                           0.92     56962\n",
      "   macro avg       0.50      0.69      0.49     56962\n",
      "weighted avg       1.00      0.92      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', KNeighborsClassifier()]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__n_neighbors': [5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of K Nearest Neighbours (SMOTE): {cv_score}\\nTest score of K Nearest Neighbours (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e0382fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of K Nearest Neighbours (ADASYN): 0.07254824142467371\n",
      "Test score of K Nearest Neighbours (ADASYN): 0.051101731340057695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     56880\n",
      "           1       0.01      0.49      0.01        82\n",
      "\n",
      "    accuracy                           0.91     56962\n",
      "   macro avg       0.50      0.70      0.48     56962\n",
      "weighted avg       1.00      0.91      0.95     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', KNeighborsClassifier()]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__n_neighbors': [5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of K Nearest Neighbours (ADASYN): {cv_score}\\nTest score of K Nearest Neighbours (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51b3d1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of K Nearest Neighbours (NearMiss): 0.017647836320224458\n",
      "Test score of K Nearest Neighbours (NearMiss): 0.01554353956679168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40     56880\n",
      "           1       0.00      0.93      0.00        82\n",
      "\n",
      "    accuracy                           0.25     56962\n",
      "   macro avg       0.50      0.59      0.20     56962\n",
      "weighted avg       1.00      0.25      0.40     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', KNeighborsClassifier()]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__n_neighbors': [5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of K Nearest Neighbours (NearMiss): {cv_score}\\nTest score of K Nearest Neighbours (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a587a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of SVM: {'fit_time': array([41.11395431, 54.63740015, 32.14832902, 43.77386737, 16.19999695,\n",
      "       26.7181623 , 15.12197828, 15.26312017, 17.33133793, 14.31192279]), 'score_time': array([0.30522037, 0.52632189, 0.52830648, 0.30204916, 0.29819107,\n",
      "       0.29980087, 0.29664946, 0.29749727, 0.30010772, 0.30052543]), 'test_mcc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "Test score of SVM: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.00      0.00      0.00        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.50      0.50      0.50     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/projectgrps/IS450/IS450G10/jupyterlab-venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/common/home/projectgrps/IS450/IS450G10/jupyterlab-venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/common/home/projectgrps/IS450/IS450G10/jupyterlab-venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='linear').fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(svm,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "test_score = matthews_corrcoef(y_test, y_pred)\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of SVM: {cv_score}\\nTest score of SVM: {test_score}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparameter Tuning \n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "parameters = {'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of SVM: {cv_score}\\nTest score of SVM: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45c07bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of SVM (SMOTE): {'fit_time': array([131.73266554, 138.74124932, 136.51871181, 134.39846754,\n",
      "       134.24043775, 141.70250726, 153.55225468, 153.93570518,\n",
      "       156.511832  , 160.88977575]), 'score_time': array([0.03327751, 0.02469206, 0.01597786, 0.01975179, 0.0191915 ,\n",
      "       0.02161169, 0.02027321, 0.02660227, 0.02343965, 0.02087188]), 'test_mcc': array([0.10841285, 0.12625296, 0.12113263, 0.13167621, 0.1194122 ,\n",
      "       0.10948328, 0.09274165, 0.12659465, 0.12103229, 0.12557635])}\n",
      "Test score of SVM (SMOTE): 0.9423650854955935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     56880\n",
      "           1       0.02      0.70      0.03        82\n",
      "\n",
      "    accuracy                           0.94     56962\n",
      "   macro avg       0.51      0.82      0.50     56962\n",
      "weighted avg       1.00      0.94      0.97     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', LinearSVC()]])\n",
    "svm_smote_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(svm_smote_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = svm_smote_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of SVM (SMOTE): {cv_score}\\nTest score of SVM (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, svm_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a88935a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10329667616115765\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, svm_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with Hyperparameter Tuning\n",
    "svm_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', SVC(kernel='linear')]])\n",
    "\n",
    "parameters = {'classifier__kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of SVM (SMOTE): {cv_score}\\nTest score of SVM (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6377434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of SVM (ADASYN): {'fit_time': array([178.68989825, 179.67237782, 169.93565798, 180.33084536,\n",
      "       174.21558118, 175.78799725, 159.52068496, 160.23257875,\n",
      "       165.16645861, 161.52824211]), 'score_time': array([0.02683806, 0.0211668 , 0.01581526, 0.04043341, 0.01920295,\n",
      "       0.0663271 , 0.01660538, 0.02197695, 0.05087042, 0.01690125]), 'test_mcc': array([0.09508362, 0.10647642, 0.10224702, 0.11234791, 0.10368882,\n",
      "       0.09282749, 0.08752797, 0.10380828, 0.10381561, 0.10934677])}\n",
      "Test score of SVM (ADASYN): 0.9185597415821074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     56880\n",
      "           1       0.01      0.72      0.02        82\n",
      "\n",
      "    accuracy                           0.92     56962\n",
      "   macro avg       0.51      0.82      0.49     56962\n",
      "weighted avg       1.00      0.92      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', LinearSVC()]])\n",
    "svm_adasyn_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(svm_adasyn_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = svm_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of SVM (ADASYN): {cv_score}\\nTest score of SVM (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, svm_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dab08040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0881787546474945\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, svm_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcaae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with Hyperparameter Tuning\n",
    "svm_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', SVC(kernel='linear')]])\n",
    "\n",
    "parameters = {'classifier__kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of SVM (ADASYN): {cv_score}\\nTest score of SVM (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6b89906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of SVM (NearMiss): {'fit_time': array([1.3309226 , 1.37355733, 1.39299917, 1.21821475, 1.39118171,\n",
      "       1.58080244, 1.27081418, 1.42087007, 1.56391215, 1.37551737]), 'score_time': array([0.01597381, 0.02359796, 0.02109742, 0.02177191, 0.03400135,\n",
      "       0.03294444, 0.02228665, 0.02904296, 0.01948857, 0.02191877]), 'test_mcc': array([0.02418694, 0.03157277, 0.03333658, 0.0272792 , 0.03026915,\n",
      "       0.02505876, 0.02282091, 0.03130516, 0.03347676, 0.02634309])}\n",
      "Test score of SVM (NearMiss): 0.46167620518942454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.46      0.63     56880\n",
      "           1       0.00      0.88      0.00        82\n",
      "\n",
      "    accuracy                           0.46     56962\n",
      "   macro avg       0.50      0.67      0.32     56962\n",
      "weighted avg       1.00      0.46      0.63     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', LinearSVC()]])\n",
    "svm_nearmiss_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(svm_nearmiss_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = svm_nearmiss_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of SVM (NearMiss): {cv_score}\\nTest score of SVM (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, svm_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "984e6408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02579552413409107\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, svm_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ccfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparameter Tuning\n",
    "svm_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', SVC(kernel='linear')]])\n",
    "\n",
    "parameters = {'classifier__kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of SVM (NearMiss): {cv_score}\\nTest score of SVM (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba57006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of MLP: {'fit_time': array([26.35921168, 28.20704508, 26.42927217, 26.69829226, 28.43928647,\n",
      "       26.1976285 , 28.61331034, 29.05260992, 27.60587454, 25.46734118]), 'score_time': array([0.0254941 , 0.09755969, 0.02630663, 0.04212952, 0.0407753 ,\n",
      "       0.02947497, 0.04919004, 0.04068208, 0.03156877, 0.02830887]), 'test_mcc': array([0.11013807, 0.        , 0.        , 0.        , 0.15603661,\n",
      "       0.        , 0.        , 0.0897669 , 0.15576557, 0.        ])}\n",
      "Test score of MLP: 0.9986306660580738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.83      0.06      0.11        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.92      0.53      0.56     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=424, hidden_layer_sizes=(8,4,2,1), max_iter=100,activation = 'relu',solver='adam') \n",
    "mlp.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(mlp,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = mlp.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of MLP: {cv_score}\\nTest score of MLP: {test_score}')\n",
    "print(classification_report(y_test, mlp.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "308d1089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22520201370634071\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, mlp.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with Hyperparameter Tuning\n",
    "mlp = MLPClassifier(random_state=424) \n",
    "\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=mlp,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of MLP: {cv_score}\\nTest score of MLP: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f5de59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of MLP (SMOTE): {'fit_time': array([107.10541487, 317.43867946, 250.39264274, 190.07684326,\n",
      "       388.24290895, 239.55918837, 320.55907035, 127.43475103,\n",
      "       149.89980698,  86.02494192]), 'score_time': array([0.04927802, 0.04366159, 0.04502392, 0.04081869, 0.09350467,\n",
      "       0.05382109, 0.0496459 , 0.02604461, 0.03793526, 0.02878332]), 'test_mcc': array([0.12415156, 0.10978632, 0.12160147, 0.0974503 , 0.10072518,\n",
      "       0.09606673, 0.11399958, 0.10853043, 0.10299595, 0.14188924])}\n",
      "Test score of MLP (SMOTE): 0.9095537375794389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     56880\n",
      "           1       0.01      0.78      0.02        82\n",
      "\n",
      "    accuracy                           0.91     56962\n",
      "   macro avg       0.51      0.85      0.49     56962\n",
      "weighted avg       1.00      0.91      0.95     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', MLPClassifier(random_state=424, hidden_layer_sizes=(8,4,2,1), max_iter=100,activation = 'relu',solver='adam')]])\n",
    "\n",
    "\n",
    "mlp_smote_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(mlp_smote_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = mlp_smote_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of MLP (SMOTE): {cv_score}\\nTest score of MLP (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, mlp_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72e7c855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09087553791518724\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, mlp_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5abc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with Hyperparameter Tuning\n",
    "mlp_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', MLPClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__hidden_layer_sizes': [(50,50,50), (50,100,50)],\n",
    "    'classifier__activation': ['tanh', 'relu'],\n",
    "    'classifier__learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=mlp_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of MLP (SMOTE): {cv_score}\\nTest score of MLP (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63632d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of MLP (ADASYN): {'fit_time': array([311.45873165, 273.96544957, 164.99931431, 195.15689373,\n",
      "        75.25734019, 254.67354107, 323.4084487 ,  66.51751685,\n",
      "       335.6289413 ,  77.47228074]), 'score_time': array([0.06489611, 0.05826569, 0.03893471, 0.04498339, 0.05435038,\n",
      "       0.03512478, 0.04316974, 0.04015589, 0.03309035, 0.03779864]), 'test_mcc': array([ 0.08145929,  0.10359438,  0.08908964,  0.07861742,  0.00393452,\n",
      "        0.08786146,  0.06865695, -0.00541157,  0.08593239,  0.0042308 ])}\n",
      "Test score of MLP (ADASYN): 0.9210701871423054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     56880\n",
      "           1       0.01      0.78      0.03        82\n",
      "\n",
      "    accuracy                           0.92     56962\n",
      "   macro avg       0.51      0.85      0.49     56962\n",
      "weighted avg       1.00      0.92      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', MLPClassifier(random_state=424, hidden_layer_sizes=(8,4,2,1), max_iter=100,activation = 'relu',solver='adam')]])\n",
    "\n",
    "mlp_adasyn_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(mlp_adasyn_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = mlp_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of MLP (ADASYN): {cv_score}\\nTest score of MLP (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, mlp_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f599530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.098220844816082\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, mlp_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with Hyperparameter Tuning\n",
    "mlp_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', MLPClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__hidden_layer_sizes': [(50,50,50), (50,100,50)],\n",
    "    'classifier__activation': ['tanh', 'relu'],\n",
    "    'classifier__learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=mlp_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of MLP (ADASYN): {cv_score}\\nTest score of MLP (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a6b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of MLP (NearMiss): {'fit_time': array([2.24240494, 2.19154   , 2.00501537, 2.04783273, 1.65226626,\n",
      "       1.70742679, 2.07145739, 1.89687943, 2.35023594, 2.02746058]), 'score_time': array([0.02269769, 0.06175447, 0.02091074, 0.02681136, 0.02033925,\n",
      "       0.03054953, 0.03097129, 0.03048801, 0.02595901, 0.04256058]), 'test_mcc': array([ 0.00228842, -0.0240065 , -0.02364337, -0.02263154,  0.00159226,\n",
      "        0.00208862,  0.00184628, -0.04273823, -0.03972067, -0.02038354])}\n",
      "Test score of MLP (NearMiss): 0.0036866683051859136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00     56880\n",
      "           1       0.00      1.00      0.00        82\n",
      "\n",
      "    accuracy                           0.00     56962\n",
      "   macro avg       0.50      0.50      0.00     56962\n",
      "weighted avg       1.00      0.00      0.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', MLPClassifier(random_state=424, hidden_layer_sizes=(8,4,2,1), max_iter=100,activation = 'relu',solver='adam')]])\n",
    "mlp_nearmiss_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(mlp_nearmiss_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = mlp_nearmiss_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of MLP (NearMiss): {cv_score}\\nTest score of MLP (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, mlp_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "589b35e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001801888882307406\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, mlp_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874540a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with Hyperparameter Tuning\n",
    "mlp_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', MLPClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__hidden_layer_sizes': [(50,50,50), (50,100,50)],\n",
    "    'classifier__activation': ['tanh', 'relu'],\n",
    "    'classifier__learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=mlp_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of MLP (NearMiss): {cv_score}\\nTest score of MLP (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d048e657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of XGBoost: {'fit_time': array([2.05303431, 2.37749815, 4.66863036, 2.25038028, 1.73118162,\n",
      "       1.4298377 , 1.92157388, 2.52977967, 2.16593194, 1.17691875]), 'score_time': array([0.04138827, 0.05415344, 0.0833919 , 0.03343534, 0.04764915,\n",
      "       0.03501487, 0.06186271, 0.05607581, 0.05546021, 0.02990341]), 'test_mcc': array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "       -0.00039782,  0.        ,  0.        ,  0.        ,  0.        ])}\n",
      "Test score of XGBoost: 0.9985604438046417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.00      0.00      0.00        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.50      0.50      0.50     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "xg_clf = XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "xg_clf.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(xg_clf,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = xg_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of XGBoost: {cv_score}\\nTest score of XGBoost: {test_score}')\n",
    "print(classification_report(y_test, xg_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1c00024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, xg_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed685fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparameter Tuning \n",
    "xg_clf = XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "\n",
    "parameters = {\n",
    " 'max_depth':[10,20,30]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xg_clf,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of XGBoost: {cv_score}\\nTest score of XGBoost: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dd13934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of XGBoost (SMOTE): {'fit_time': array([4.3858037 , 5.60655975, 5.61453032, 4.94328356, 5.79736423,\n",
      "       8.0923152 , 6.15528274, 5.74961972, 4.89231801, 6.17270708]), 'score_time': array([0.04012275, 0.04482627, 0.04224706, 0.03225541, 0.03345037,\n",
      "       0.04791307, 0.03762031, 0.06389785, 0.03427219, 0.04764032]), 'test_mcc': array([0.11375549, 0.1211061 , 0.10010244, 0.11513084, 0.10530634,\n",
      "       0.09360949, 0.0839693 , 0.11269784, 0.10873759, 0.11627335])}\n",
      "Test score of XGBoost (SMOTE): 0.9209999648888733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     56880\n",
      "           1       0.01      0.68      0.02        82\n",
      "\n",
      "    accuracy                           0.92     56962\n",
      "   macro avg       0.51      0.80      0.49     56962\n",
      "weighted avg       1.00      0.92      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "xgb_smote_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(xgb_smote_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = xgb_smote_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of XGBoost (SMOTE): {cv_score}\\nTest score of XGBoost (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, xgb_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56f658e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08467797158872979\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, xgb_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparameter Tuning \n",
    "xgb_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "\n",
    "parameters = {\n",
    " 'classifier__max_depth':[10,20,30]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xgb_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of XGBoost (SMOTE): {cv_score}\\nTest score of XGBoost (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2262d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of XGBoost (ADASYN): {'fit_time': array([7.73514795, 5.28440809, 6.03326416, 8.88621974, 5.50609708,\n",
      "       4.05940866, 5.3144455 , 7.1686244 , 6.73967195, 5.37842298]), 'score_time': array([0.03911233, 0.05741191, 0.08582807, 0.03940439, 0.03606415,\n",
      "       0.03411198, 0.04244637, 0.05395818, 0.0620532 , 0.04700971]), 'test_mcc': array([0.09880723, 0.10897914, 0.08804404, 0.10487642, 0.09439104,\n",
      "       0.08668179, 0.07405228, 0.09437964, 0.09825867, 0.09992563])}\n",
      "Test score of XGBoost (ADASYN): 0.8998806221691654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95     56880\n",
      "           1       0.01      0.70      0.02        82\n",
      "\n",
      "    accuracy                           0.90     56962\n",
      "   macro avg       0.50      0.80      0.48     56962\n",
      "weighted avg       1.00      0.90      0.95     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "xgb_adasyn_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(xgb_adasyn_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = xgb_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of XGBoost (ADASYN): {cv_score}\\nTest score of XGBoost (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, xgb_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef2552da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07500751814462484\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, xgb_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparameter Tuning \n",
    "xgb_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "\n",
    "parameters = {\n",
    " 'classifier__max_depth':[10,20,30]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xgb_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of XGBoost (ADASYN): {cv_score}\\nTest score of XGBoost (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69ea9dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:43:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of XGBoost (NearMiss): {'fit_time': array([7.62330365, 6.89057279, 5.50733638, 5.63786125, 7.58895898,\n",
      "       9.19501114, 7.83133316, 6.2467823 , 4.85908413, 7.25825787]), 'score_time': array([0.0424602 , 0.05552936, 0.03632641, 0.06972218, 0.09154773,\n",
      "       0.06530309, 0.08440828, 0.0479877 , 0.03355956, 0.05724168]), 'test_mcc': array([0.09880723, 0.10897914, 0.08804404, 0.10487642, 0.09439104,\n",
      "       0.08668179, 0.07405228, 0.09437964, 0.09825867, 0.09992563])}\n",
      "Test score of XGBoost (NearMiss): 0.8998806221691654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95     56880\n",
      "           1       0.01      0.70      0.02        82\n",
      "\n",
      "    accuracy                           0.90     56962\n",
      "   macro avg       0.50      0.80      0.48     56962\n",
      "weighted avg       1.00      0.90      0.95     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_nearmiss_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "xgb_nearmiss_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(xgb_nearmiss_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = xgb_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of XGBoost (NearMiss): {cv_score}\\nTest score of XGBoost (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, xgb_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fdc6d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07500751814462484\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, xgb_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b17818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparameter Tuning \n",
    "xgb_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "\n",
    "parameters = {\n",
    " 'classifier__max_depth':[10,20,30]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xgb_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of XGBoost (NearMiss): {cv_score}\\nTest score of XGBoost (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aa4cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of XGBoost: {'fit_time': array([ 24.40436816,   7.17787743,  32.76145911,   7.63490391,\n",
      "        43.19109654,  72.17870474,  79.60608745, 108.20109868,\n",
      "        75.10900331,  38.78671551]), 'score_time': array([0.13536716, 0.10960937, 0.10687137, 0.1208961 , 0.14543009,\n",
      "       0.30749607, 0.25912547, 0.26845121, 0.59515214, 0.27574134]), 'test_mcc': array([ 0.0298729 , -0.00101445,  0.03297057, -0.0005626 ,  0.02863815,\n",
      "        0.06928458,  0.06654267,  0.0693856 ,  0.05873866, -0.00171241])}\n",
      "Test score of XGBoost: 0.9979635546504687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.05      0.02      0.03        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.53      0.51      0.52     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm.Dataset(X, categorical_feature='Class')\n",
    "lgb_clf=lgbm.LGBMClassifier()\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(lgb_clf,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = lgb_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of XGBoost: {cv_score}\\nTest score of XGBoost: {test_score}')\n",
    "print(classification_report(y_test, lgb_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6fd8db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03488549525923992\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, lgb_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5837a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of LGBM (SMOTE): {'fit_time': array([ 67.99890709, 170.49914312, 102.16797876,  66.1018908 ,\n",
      "        87.53721476,  53.75765085,  72.0162077 ,  67.23905587,\n",
      "        52.34940743,  42.58186913]), 'score_time': array([0.21127367, 0.36198902, 0.36124063, 0.28398299, 0.37575817,\n",
      "       0.1782167 , 0.33850622, 0.21667886, 0.23488641, 0.11014581]), 'test_mcc': array([0.10360918, 0.1058193 , 0.0880283 , 0.098804  , 0.09146305,\n",
      "       0.09970819, 0.08111531, 0.09826413, 0.11097282, 0.09564496])}\n",
      "Test score of LGBM (SMOTE): 0.9255995224886766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96     56880\n",
      "           1       0.01      0.68      0.03        82\n",
      "\n",
      "    accuracy                           0.93     56962\n",
      "   macro avg       0.51      0.80      0.49     56962\n",
      "weighted avg       1.00      0.93      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm.Dataset(X, categorical_feature='Class')\n",
    "lgb_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['lgbm', lgbm.LGBMClassifier()]])\n",
    "\n",
    "\n",
    "lgb_smote_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(lgb_smote_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = lgb_smote_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of LGBM (SMOTE): {cv_score}\\nTest score of LGBM (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, lgb_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab3b9eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08768451961906312\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, lgb_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b21bb07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of LGBM (ADASYN): {'fit_time': array([16.00663567, 15.75337029,  6.00889111,  6.2485528 , 11.03545403,\n",
      "        7.39267325,  6.76264   ,  7.85567713,  7.40237117,  5.42413187]), 'score_time': array([0.1102705 , 0.09884477, 0.09758306, 0.11727571, 0.09564209,\n",
      "       0.1080811 , 0.09698844, 0.10135913, 0.12335443, 0.10445666]), 'test_mcc': array([0.09035666, 0.09411203, 0.08315674, 0.09844712, 0.10223114,\n",
      "       0.08465255, 0.07454254, 0.09486595, 0.0957686 , 0.08597713])}\n",
      "Test score of LGBM (ADASYN): 0.9060952915979074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     56880\n",
      "           1       0.01      0.73      0.02        82\n",
      "\n",
      "    accuracy                           0.91     56962\n",
      "   macro avg       0.51      0.82      0.49     56962\n",
      "weighted avg       1.00      0.91      0.95     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm.Dataset(X, categorical_feature='Class')\n",
    "lgb_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['lgbm', lgbm.LGBMClassifier()]])\n",
    "\n",
    "\n",
    "lgb_adasyn_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(lgb_adasyn_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = lgb_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of LGBM (ADASYN): {cv_score}\\nTest score of LGBM (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, lgb_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07f3784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08267059391662826\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, lgb_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2e4e80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of LGBM (Nearmiss): {'fit_time': array([2.01474643, 1.46986532, 1.31254649, 1.37675571, 1.83099771,\n",
      "       1.35161924, 1.36816907, 1.24567461, 1.80551505, 1.72812891]), 'score_time': array([0.0812583 , 0.07642078, 0.09621143, 0.0744803 , 0.08291841,\n",
      "       0.07088017, 0.07411718, 0.08916283, 0.09204173, 0.08550334]), 'test_mcc': array([ 0.00942476,  0.00968978,  0.00863494,  0.00579713,  0.00250917,\n",
      "       -0.00515856,  0.00501916,  0.00120372,  0.00869971,  0.00556644])}\n",
      "Test score of LGBM (Nearmiss): 0.050858467048207574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.05      0.09     56880\n",
      "           1       0.00      0.98      0.00        82\n",
      "\n",
      "    accuracy                           0.05     56962\n",
      "   macro avg       0.50      0.51      0.05     56962\n",
      "weighted avg       1.00      0.05      0.09     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm.Dataset(X, categorical_feature='Class')\n",
    "lgb_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['lgbm', lgbm.LGBMClassifier()]])\n",
    "\n",
    "\n",
    "lgb_nearmiss_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(lgb_nearmiss_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = lgb_nearmiss_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of LGBM (Nearmiss): {cv_score}\\nTest score of LGBM (Nearmiss): {test_score}')\n",
    "print(classification_report(y_test, lgb_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d504aed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004393879752235725\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, lgb_nearmiss_pipeline.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
