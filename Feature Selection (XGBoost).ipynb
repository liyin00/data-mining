{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac57bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77c7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE, ADASYN \n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#from sklearn import cross_validation, linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#For Decision Tree implementation\n",
    "from scipy.stats import entropy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#For KNN implementation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For NB implementation \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "#For Logistic Regression implementation \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#For Random Forest implementation \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#For SVM implementation \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#For MLP implementation \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#For XGBoost \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#For LightGBM \n",
    "import lightgbm as lgbm\n",
    "\n",
    "#For Stacking \n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2bf93b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 14, 7\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4530c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53148c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/projectgrps/IS450/IS450G10/jupyterlab-venv/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:01:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Feature importance based on XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_importances = pd.DataFrame(data={\n",
    "    'Attribute': X_train.columns,\n",
    "    'Importance': xgb.feature_importances_\n",
    "})\n",
    "xgb_importances = xgb_importances.sort_values(by='Importance', ascending=False)\n",
    "plt.bar(x=xgb_importances['Attribute'], height=xgb_importances['Importance'], color='#087E8B')\n",
    "plt.title('Feature importances obtained from coefficients', size=20)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc023c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56ccf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.groupby('Class', group_keys=False).apply(lambda x: x.sample(100))\n",
    "stratified_kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=424)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bb375",
   "metadata": {},
   "source": [
    "## XGBoost Top 2 Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e244c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[['V17', 'V14']], df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9778568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Logistic Regression: 0.7055396250680397\n",
      "Test score of Logistic Regression: 0.6952763641140478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.79      0.61      0.69        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.90      0.80      0.84     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=424)\n",
    "\n",
    "c_values = [0.1, 10, 100]\n",
    "parameters = dict(C=c_values)\n",
    "grid_search = GridSearchCV(estimator=log_reg,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Logistic Regression: {cv_score}\\nTest score of Logistic Regression: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f76b177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Logistic Regression (SMOTE): 0.2413118756665063\n",
      "Test score of Logistic Regression (SMOTE): 0.22776723146241087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     56880\n",
      "           1       0.06      0.89      0.11        82\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.53      0.94      0.55     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', LogisticRegression(random_state=424)]])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {'classifier__C':[0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=log_reg_smote_pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Logistic Regression (SMOTE): {cv_score}\\nTest score of Logistic Regression (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5bf616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Logistic Regression (ADASYN): 0.11681052823315632\n",
      "Test score of Logistic Regression (ADASYN): 0.10839546424097965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     56880\n",
      "           1       0.01      0.93      0.03        82\n",
      "\n",
      "    accuracy                           0.91     56962\n",
      "   macro avg       0.51      0.92      0.49     56962\n",
      "weighted avg       1.00      0.91      0.95     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', LogisticRegression(random_state=424)]])\n",
    "\n",
    "\n",
    "param_grid = {'classifier__C':[0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=log_reg_adasyn_pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Logistic Regression (ADASYN): {cv_score}\\nTest score of Logistic Regression (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0004d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Logistic Regression (NearMiss): 0.21260729195404243\n",
      "Test score of Logistic Regression (NearMiss): 0.1300504736439731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     56880\n",
      "           1       0.02      0.91      0.04        82\n",
      "\n",
      "    accuracy                           0.94     56962\n",
      "   macro avg       0.51      0.93      0.50     56962\n",
      "weighted avg       1.00      0.94      0.97     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', LogisticRegression(random_state=424)]])\n",
    "\n",
    "\n",
    "param_grid = {'classifier__C':[0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=log_reg_nearmiss_pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Logistic Regression (NearMiss): {cv_score}\\nTest score of Logistic Regression (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a48b92bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Naive Bayes: 0.6818061100478952\n",
      "Test score of Naive Bayes: 0.639260874096992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.52      0.78      0.63        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.76      0.89      0.81     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "parameters = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "grid_search = GridSearchCV(estimator=nb,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Naive Bayes: {cv_score}\\nTest score of Naive Bayes: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c02e86d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Naive Bayes (SMOTE): 0.6883690681602028\n",
      "Test score of Naive Bayes (SMOTE): 0.6489058878962302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.79      0.54      0.64        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.89      0.77      0.82     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                   ['classifier', GaussianNB()]])\n",
    "\n",
    "parameters = {'classifier__var_smoothing': [0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=nb_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Naive Bayes (SMOTE): {cv_score}\\nTest score of Naive Bayes (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49150dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Naive Bayes (ADASYN): 0.7396864102048126\n",
      "Test score of Naive Bayes (ADASYN): 0.7140670248191661\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.70      0.73      0.71        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.85      0.87      0.86     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                   ['classifier', GaussianNB()]])\n",
    "\n",
    "parameters = {'classifier__var_smoothing': [0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=nb_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Naive Bayes (ADASYN): {cv_score}\\nTest score of Naive Bayes (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7856aefa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Naive Bayes (NearMiss): 0.6693728578404138\n",
      "Test score of Naive Bayes (NearMiss): 0.6215055728143746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.77      0.50      0.61        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.89      0.75      0.80     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                   ['classifier', GaussianNB()]])\n",
    "\n",
    "parameters = {'classifier__var_smoothing': [0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(estimator=nb_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Naive Bayes (NearMiss): {cv_score}\\nTest score of Naive Bayes (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a84bbb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Decision Tree: 0.7713430847577861\n",
      "Test score of Decision Tree: 0.739230990183918\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.77      0.71      0.74        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.89      0.85      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt= DecisionTreeClassifier(random_state=424)\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_leaf': [5, 20, 50],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Decision Tree: {cv_score}\\nTest score of Decision Tree: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f58e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Decision Tree (SMOTE): 0.24932714898440378\n",
      "Test score of Decision Tree (SMOTE): 0.22755871746278028\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     56880\n",
      "           1       0.06      0.85      0.12        82\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.53      0.92      0.55     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', DecisionTreeClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__min_samples_leaf': [5, 20, 50],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Decision Tree (SMOTE): {cv_score}\\nTest score of Decision Tree (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0d8e89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Decision Tree (ADASYN): 0.1368298396141811\n",
      "Test score of Decision Tree (ADASYN): 0.12625039248832837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97     56880\n",
      "           1       0.02      0.90      0.04        82\n",
      "\n",
      "    accuracy                           0.93     56962\n",
      "   macro avg       0.51      0.92      0.50     56962\n",
      "weighted avg       1.00      0.93      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', DecisionTreeClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__min_samples_leaf': [5, 20, 50],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Decision Tree (ADASYN): {cv_score}\\nTest score of Decision Tree (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6835b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Decision Tree (NearMiss): 0.059501431156546734\n",
      "Test score of Decision Tree (NearMiss): 0.03580065060995711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.66     56880\n",
      "           1       0.00      0.98      0.01        82\n",
      "\n",
      "    accuracy                           0.50     56962\n",
      "   macro avg       0.50      0.74      0.33     56962\n",
      "weighted avg       1.00      0.50      0.66     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', DecisionTreeClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__min_samples_leaf': [5, 20, 50],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Decision Tree (NearMiss): {cv_score}\\nTest score of Decision Tree (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7da70b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Random Forest: 0.7843820556100234\n",
      "Test score of Random Forest: 0.774526033844184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.86      0.70      0.77        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.93      0.85      0.88     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=424)\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Random Forest: {cv_score}\\nTest score of Random Forest: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eee15e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Random Forest (SMOTE): 0.2776936304566698\n",
      "Test score of Random Forest (SMOTE): 0.24796565039396454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56880\n",
      "           1       0.08      0.83      0.14        82\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.54      0.91      0.57     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', RandomForestClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Random Forest (SMOTE): {cv_score}\\nTest score of Random Forest (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50aebd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Random Forest (ADASYN): 0.1540676053600401\n",
      "Test score of Random Forest (ADASYN): 0.12583126722013369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     56880\n",
      "           1       0.02      0.78      0.04        82\n",
      "\n",
      "    accuracy                           0.95     56962\n",
      "   macro avg       0.51      0.87      0.51     56962\n",
      "weighted avg       1.00      0.95      0.97     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', RandomForestClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Random Forest (ADASYN): {cv_score}\\nTest score of Random Forest (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e027334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of Random Forest (NearMiss): 0.010762872242072157\n",
      "Test score of Random Forest (NearMiss): 0.005841265499842168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.05     56880\n",
      "           1       0.00      1.00      0.00        82\n",
      "\n",
      "    accuracy                           0.02     56962\n",
      "   macro avg       0.50      0.51      0.02     56962\n",
      "weighted avg       1.00      0.02      0.05     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', RandomForestClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of Random Forest (NearMiss): {cv_score}\\nTest score of Random Forest (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d5aada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of K Nearest Neighbours: 0.7883772264791161\n",
      "Test score of K Nearest Neighbours: 0.7530439867048621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.85      0.67      0.75        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.92      0.84      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_neighbors': [5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of K Nearest Neighbours: {cv_score}\\nTest score of K Nearest Neighbours: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5989d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of K Nearest Neighbours (SMOTE): 0.18431729303232167\n",
      "Test score of K Nearest Neighbours (SMOTE): 0.16204193333462022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     56880\n",
      "           1       0.03      0.84      0.06        82\n",
      "\n",
      "    accuracy                           0.96     56962\n",
      "   macro avg       0.52      0.90      0.52     56962\n",
      "weighted avg       1.00      0.96      0.98     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', KNeighborsClassifier()]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__n_neighbors': [5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of K Nearest Neighbours (SMOTE): {cv_score}\\nTest score of K Nearest Neighbours (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ab947d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of K Nearest Neighbours (ADASYN): 0.13132396552484404\n",
      "Test score of K Nearest Neighbours (ADASYN): 0.11365152973616707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97     56880\n",
      "           1       0.02      0.82      0.03        82\n",
      "\n",
      "    accuracy                           0.93     56962\n",
      "   macro avg       0.51      0.88      0.50     56962\n",
      "weighted avg       1.00      0.93      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', KNeighborsClassifier()]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__n_neighbors': [5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of K Nearest Neighbours (ADASYN): {cv_score}\\nTest score of K Nearest Neighbours (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b58a9ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score of K Nearest Neighbours (NearMiss): 0.03292068626404846\n",
      "Test score of K Nearest Neighbours (NearMiss): 0.0266654205051482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.35      0.52     56880\n",
      "           1       0.00      0.99      0.00        82\n",
      "\n",
      "    accuracy                           0.35     56962\n",
      "   macro avg       0.50      0.67      0.26     56962\n",
      "weighted avg       1.00      0.35      0.51     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', KNeighborsClassifier()]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__n_neighbors': [5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of K Nearest Neighbours (NearMiss): {cv_score}\\nTest score of K Nearest Neighbours (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907f26ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of SVM (SMOTE): {'fit_time': array([11.19920087,  8.73548818, 10.895015  ,  9.8146596 , 10.91602802,\n",
      "       10.31730509, 39.65924788,  9.15676999, 11.01161647,  9.38745427]), 'score_time': array([0.22225976, 0.22385502, 0.23494697, 0.23310924, 0.22920156,\n",
      "       0.22731376, 0.2390995 , 0.22287059, 0.23794174, 0.23685622]), 'test_mcc': array([0.66903316, 0.74501657, 0.66903316, 0.79659897, 0.7536636 ,\n",
      "       0.65085598, 0.66552917, 0.600549  , 0.65085598, 0.72887747])}\n",
      "Test score of SVM (SMOTE): 0.6578179949220128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.79      0.55      0.65        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.89      0.77      0.82     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='linear').fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(svm,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "test_score = matthews_corrcoef(y_test, y_pred)\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of SVM (SMOTE): {cv_score}\\nTest score of SVM (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code with Hyperparamter Tuning\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "parameters = {'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "print(f'Cross-validation score of SVM (SMOTE): {cv_score}\\nTest score of SVM (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22beae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of SVM (SMOTE): {'fit_time': array([111.71050882, 114.42131567, 117.6928947 , 121.30102348,\n",
      "       118.55675983, 118.55265617, 119.44888544, 121.32017183,\n",
      "       124.07896447, 119.40153503]), 'score_time': array([0.02585053, 0.0431335 , 0.0245409 , 0.03081608, 0.01596427,\n",
      "       0.02823806, 0.01666713, 0.01445103, 0.01854706, 0.02285981]), 'test_mcc': array([0.26972515, 0.28222529, 0.30362991, 0.28690581, 0.31852287,\n",
      "       0.29868859, 0.28152839, 0.29025453, 0.28773232, 0.27976481])}\n",
      "Test score of SVM (SMOTE): 0.9861486605105158\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56880\n",
      "           1       0.08      0.88      0.15        82\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.54      0.93      0.57     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', LinearSVC()]])\n",
    "svm_smote_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(svm_smote_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = svm_smote_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of SVM (SMOTE): {cv_score}\\nTest score of SVM (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, svm_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20dff99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27014020222400326\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, svm_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b96e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with Hyperparameter Tuning\n",
    "svm_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', SVC(kernel='linear')]])\n",
    "\n",
    "parameters = {'classifier__kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of SVM (SMOTE): {cv_score}\\nTest score of SVM (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a94cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of SVM (ADASYN): {'fit_time': array([156.3342185 , 155.95192599, 158.69832969, 145.93547702,\n",
      "       144.31940556, 158.94089651, 146.59657788, 135.81937432,\n",
      "       133.79557562, 130.67130804]), 'score_time': array([0.02087641, 0.01824498, 0.04236484, 0.02442551, 0.03240347,\n",
      "       0.0497942 , 0.01867342, 0.02239323, 0.01890063, 0.02226901]), 'test_mcc': array([0.14407083, 0.12804521, 0.12667573, 0.13106116, 0.13441631,\n",
      "       0.12608536, 0.14573744, 0.12660048, 0.13175789, 0.12132444])}\n",
      "Test score of SVM (ADASYN): 0.9266528562901584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96     56880\n",
      "           1       0.02      0.91      0.03        82\n",
      "\n",
      "    accuracy                           0.93     56962\n",
      "   macro avg       0.51      0.92      0.50     56962\n",
      "weighted avg       1.00      0.93      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', LinearSVC()]])\n",
    "svm_adasyn_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(svm_adasyn_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = svm_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of SVM (ADASYN): {cv_score}\\nTest score of SVM (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, svm_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a563935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12144471616277028\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, svm_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8997727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code with Hyperparamter Tuning\n",
    "svm_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', SVC(kernel='linear')]])\n",
    "\n",
    "parameters = {'classifier__kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of SVM (ADASYN): {cv_score}\\nTest score of SVM (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62091a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of SVM (NearMiss): {'fit_time': array([1.36895394, 1.4258225 , 1.30185151, 1.21537971, 1.23107791,\n",
      "       1.40309405, 1.31588554, 1.19048357, 1.23958158, 1.33856797]), 'score_time': array([0.02202439, 0.02209473, 0.01519442, 0.01277781, 0.02024913,\n",
      "       0.02105856, 0.0195694 , 0.01711798, 0.01862454, 0.01799202]), 'test_mcc': array([0.12344726, 0.5586537 , 0.11667879, 0.11555203, 0.11484636,\n",
      "       0.1104304 , 0.12869549, 0.1052641 , 0.12121837, 0.52456755])}\n",
      "Test score of SVM (NearMiss): 0.9094484041992907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     56880\n",
      "           1       0.01      0.93      0.03        82\n",
      "\n",
      "    accuracy                           0.91     56962\n",
      "   macro avg       0.51      0.92      0.49     56962\n",
      "weighted avg       1.00      0.91      0.95     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', LinearSVC()]])\n",
    "svm_nearmiss_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(svm_nearmiss_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = svm_nearmiss_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of SVM (NearMiss): {cv_score}\\nTest score of SVM (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, svm_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38bd9788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10981670036245596\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, svm_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code with Hyperparamter Tuning\n",
    "svm_nearmiss_pipeline = imbpipeline(steps = [['adasyn', NearMiss()],\n",
    "                                        ['classifier', SVC(kernel='linear')]])\n",
    "\n",
    "parameters = {'classifier__kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of SVM (NearMiss): {cv_score}\\nTest score of SVM (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0048df4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of MLP: {'fit_time': array([27.82755208, 28.23275137, 29.11865592, 27.7424736 , 31.77797532,\n",
      "       27.52830696, 28.03951406, 28.56475163, 30.26480508, 27.70373988]), 'score_time': array([0.0392499 , 0.03842044, 0.03790307, 0.04346585, 0.03528571,\n",
      "       0.03380895, 0.04283047, 0.02936506, 0.03701043, 0.04775858]), 'test_mcc': array([0.76951877, 0.75962643, 0.76985257, 0.83326281, 0.75295093,\n",
      "       0.6733774 , 0.73871859, 0.6958797 , 0.80032092, 0.74953517])}\n",
      "Test score of MLP: 0.9992451107756047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.77      0.67      0.72        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.89      0.84      0.86     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=424, hidden_layer_sizes=(8,4,2,1), max_iter=100,activation = 'relu',solver='adam') \n",
    "mlp.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(mlp,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = mlp.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of MLP: {cv_score}\\nTest score of MLP: {test_score}')\n",
    "print(classification_report(y_test, mlp.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55e054ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7204476428060298\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, mlp.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a8ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code with Hyperparamter Tuning\n",
    "mlp = MLPClassifier(random_state=424) \n",
    "\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'learning_rate': ['constant','adaptive']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=mlp,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of MLP: {cv_score}\\nTest score of MLP: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9eb9976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of MLP (SMOTE): {'fit_time': array([106.91664147, 145.78784156, 124.45764947, 263.22190928,\n",
      "       137.14723468,  92.28412127, 166.21550131, 129.7397275 ,\n",
      "       215.44585752,  88.82095504]), 'score_time': array([0.03755665, 0.04735351, 0.04392695, 0.05712152, 0.05353785,\n",
      "       0.04888248, 0.03856826, 0.04762125, 0.04026961, 0.03476238]), 'test_mcc': array([0.26832813, 0.23310907, 0.21542544, 0.2129045 , 0.23815894,\n",
      "       0.18766715, 0.2093066 , 0.21585227, 0.23494739, 0.19145646])}\n",
      "Test score of MLP (SMOTE): 0.970682209192093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56880\n",
      "           1       0.04      0.93      0.08        82\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.52      0.95      0.53     56962\n",
      "weighted avg       1.00      0.97      0.98     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', MLPClassifier(random_state=424, hidden_layer_sizes=(8,4,2,1), max_iter=100,activation = 'relu',solver='adam')]])\n",
    "\n",
    "\n",
    "mlp_smote_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(mlp_smote_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = mlp_smote_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of MLP (SMOTE): {cv_score}\\nTest score of MLP (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, mlp_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01c1eb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19775461679944956\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, mlp_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c394be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code with Hyperparamter Tuning\n",
    "mlp_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', MLPClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__hidden_layer_sizes': [(50,50,50), (50,100,50)],\n",
    "    'classifier__activation': ['tanh', 'relu'],\n",
    "    'classifier__learning_rate': ['constant','adaptive']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=mlp_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of MLP (SMOTE): {cv_score}\\nTest score of MLP (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac8a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of MLP (ADASYN): {'fit_time': array([201.90483665, 269.43234372, 186.05324435, 212.29370832,\n",
      "       163.02588892, 172.89698625, 157.00515461, 218.39372873,\n",
      "       217.7708056 , 131.79111099]), 'score_time': array([0.05479193, 0.04825687, 0.05016327, 0.08757305, 0.04110765,\n",
      "       0.058851  , 0.06954455, 0.05392027, 0.05318356, 0.04364586]), 'test_mcc': array([0.11690524, 0.09651063, 0.12964267, 0.13198153, 0.12556109,\n",
      "       0.11520996, 0.11190914, 0.09468884, 0.11115413, 0.1158216 ])}\n",
      "Test score of MLP (ADASYN): 0.8500403777957235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92     56880\n",
      "           1       0.01      0.93      0.02        82\n",
      "\n",
      "    accuracy                           0.85     56962\n",
      "   macro avg       0.50      0.89      0.47     56962\n",
      "weighted avg       1.00      0.85      0.92     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', MLPClassifier(random_state=424, hidden_layer_sizes=(8,4,2,1), max_iter=100,activation = 'relu',solver='adam')]])\n",
    "\n",
    "mlp_adasyn_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(mlp_adasyn_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = mlp_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of MLP (ADASYN): {cv_score}\\nTest score of MLP (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, mlp_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a0d86f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08220963752958461\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, mlp_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fffde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code with Hyperparamter Tuning\n",
    "mlp_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', MLPClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__hidden_layer_sizes': [(50,50,50), (50,100,50)],\n",
    "    'classifier__activation': ['tanh', 'relu'],\n",
    "    'classifier__learning_rate': ['constant','adaptive']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=mlp_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of MLP (ADASYN): {cv_score}\\nTest score of MLP (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02166406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of MLP (NearMiss): {'fit_time': array([1.93977499, 1.89469695, 1.77359223, 1.91282511, 1.86758113,\n",
      "       1.8002274 , 1.62941122, 1.62783813, 1.62308121, 1.57774949]), 'score_time': array([0.0355649 , 0.02933717, 0.02676105, 0.03076386, 0.03444409,\n",
      "       0.02629232, 0.02166438, 0.02873731, 0.03457761, 0.0227108 ]), 'test_mcc': array([-0.03054568, -0.03202251, -0.02923824,  0.00143506,  0.0013787 ,\n",
      "        0.00151575,  0.0013497 , -0.0184063 , -0.05657496, -0.03283672])}\n",
      "Test score of MLP (NearMiss): 0.0025631122502721114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.00      0.00     56880\n",
      "           1       0.00      0.98      0.00        82\n",
      "\n",
      "    accuracy                           0.00     56962\n",
      "   macro avg       0.49      0.49      0.00     56962\n",
      "weighted avg       0.97      0.00      0.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', MLPClassifier(random_state=424, hidden_layer_sizes=(8,4,2,1), max_iter=100,activation = 'relu',solver='adam')]])\n",
    "mlp_nearmiss_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(mlp_nearmiss_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = mlp_nearmiss_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of MLP (NearMiss): {cv_score}\\nTest score of MLP (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, mlp_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e040dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.025506242737020794\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, mlp_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbfd509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparamater Tuning\n",
    "mlp_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', MLPClassifier(random_state=424)]])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__hidden_layer_sizes': [(50,50,50), (50,100,50)],\n",
    "    'classifier__activation': ['tanh', 'relu'],\n",
    "    'classifier__learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=mlp_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of MLP (NearMiss): {cv_score}\\nTest score of MLP (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af844056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of XGBoost: {'fit_time': array([1.4176445 , 1.77319169, 1.31248426, 1.92973781, 1.8785646 ,\n",
      "       1.6685214 , 1.41145921, 1.36171079, 1.60417438, 1.59061527]), 'score_time': array([0.04160094, 0.0355432 , 0.02656007, 0.05114746, 0.05546069,\n",
      "       0.03491187, 0.04329777, 0.03556275, 0.02758551, 0.02361608]), 'test_mcc': array([0.68675412, 0.66075017, 0.76951877, 0.78115246, 0.73748132,\n",
      "       0.6867541 , 0.59320308, 0.65528757, 0.70405069, 0.70637812])}\n",
      "Test score of XGBoost: 0.9992099996488887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.84      0.56      0.67        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.92      0.78      0.84     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xg_clf = XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "xg_clf.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(xg_clf,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = xg_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of XGBoost: {cv_score}\\nTest score of XGBoost: {test_score}')\n",
    "print(classification_report(y_test, xg_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cdefb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6846111929893742\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, xg_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparamater Tuning\n",
    "xg_clf = XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "\n",
    "parameters = {\n",
    " 'max_depth':[10,20,30]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xg_clf,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of XGBoost: {cv_score}\\nTest score of XGBoost: {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274b6565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of XGBoost (SMOTE): {'fit_time': array([4.53891683, 4.32139397, 3.90283704, 4.12481618, 4.44846201,\n",
      "       3.99624705, 4.33271265, 4.43728209, 5.98662472, 5.18012619]), 'score_time': array([0.03111815, 0.02882743, 0.03698349, 0.03914809, 0.04091024,\n",
      "       0.02271509, 0.03291154, 0.1387043 , 0.05410957, 0.03286433]), 'test_mcc': array([0.22501216, 0.25465507, 0.25133083, 0.24538952, 0.27523124,\n",
      "       0.2441195 , 0.23995074, 0.24381787, 0.24434525, 0.24178894])}\n",
      "Test score of XGBoost (SMOTE): 0.9771777676345634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     56880\n",
      "           1       0.05      0.90      0.10        82\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.53      0.94      0.55     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "xgb_smote_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(xgb_smote_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = xgb_smote_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of XGBoost (SMOTE): {cv_score}\\nTest score of XGBoost (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, xgb_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f8221db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2180149637286369\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, xgb_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea81a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparamater Tuning\n",
    "xgb_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "\n",
    "parameters = {\n",
    " 'classifier__max_depth':[10,20,30]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xgb_smote_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of XGBoost (SMOTE): {cv_score}\\nTest score of XGBoost (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f457417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of XGBoost (ADASYN): {'fit_time': array([6.47234654, 4.81658268, 6.05293489, 6.25221276, 5.33956575,\n",
      "       5.74013615, 4.71053362, 4.17560601, 4.87743282, 4.87074327]), 'score_time': array([0.04444408, 0.041996  , 0.04512596, 0.04541397, 0.04448581,\n",
      "       0.05691934, 0.03254533, 0.02425027, 0.05006552, 0.05383921]), 'test_mcc': array([0.12678842, 0.12574688, 0.13252237, 0.15125971, 0.12257815,\n",
      "       0.12807924, 0.12839334, 0.12375733, 0.12919645, 0.12069158])}\n",
      "Test score of XGBoost (ADASYN): 0.9184544082019592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     56880\n",
      "           1       0.02      0.91      0.03        82\n",
      "\n",
      "    accuracy                           0.92     56962\n",
      "   macro avg       0.51      0.92      0.49     56962\n",
      "weighted avg       1.00      0.92      0.96     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "xgb_adasyn_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(xgb_adasyn_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = xgb_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of XGBoost (ADASYN): {cv_score}\\nTest score of XGBoost (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, xgb_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33b7d989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11465501294748073\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, xgb_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparamater Tuning\n",
    "xgb_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "\n",
    "parameters = {\n",
    " 'classifier__max_depth':[10,20,30]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xgb_adasyn_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of XGBoost (ADASYN): {cv_score}\\nTest score of XGBoost (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f337a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lingl\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of XGBoost (NearMiss): {'fit_time': array([1.58368802, 3.27993608, 1.50477242, 1.39823389, 1.86349964,\n",
      "       1.51498055, 2.08871651, 1.52201533, 1.32409477, 1.32769704]), 'score_time': array([0.05042624, 0.06268096, 0.03827405, 0.04540181, 0.04754567,\n",
      "       0.03960466, 0.03695107, 0.03186488, 0.02850056, 0.0322299 ]), 'test_mcc': array([0.01708408, 0.11204674, 0.01689291, 0.01682688, 0.0170309 ,\n",
      "       0.01610939, 0.01484498, 0.01687172, 0.01969703, 0.10408442])}\n",
      "Test score of XGBoost (NearMiss): 0.9184544082019592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.14      0.24     56880\n",
      "           1       0.00      1.00      0.00        82\n",
      "\n",
      "    accuracy                           0.14     56962\n",
      "   macro avg       0.50      0.57      0.12     56962\n",
      "weighted avg       1.00      0.14      0.24     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "xgb_nearmiss_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(xgb_nearmiss_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = xgb_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of XGBoost (NearMiss): {cv_score}\\nTest score of XGBoost (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, xgb_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4712596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015248210956041782\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, xgb_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hyperparamater Tuning\n",
    "xgb_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['classifier', XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)]])\n",
    "\n",
    "parameters = {\n",
    " 'classifier__max_depth':[10,20,30]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xgb_nearmiss_pipeline,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(matthews_corrcoef),\n",
    "                           cv=stratified_kfold, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score of XGBoost (NearMiss): {cv_score}\\nTest score of XGBoost (NearMiss): {test_score}')\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e496b9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of LBGM: {'fit_time': array([ 2.250664  ,  6.29876065, 13.04017806,  2.25111842,  2.33103156,\n",
      "       17.81078911, 33.98879528, 13.82716012,  2.76780748,  2.44460487]), 'score_time': array([0.14278746, 0.15324473, 0.14484286, 0.12373662, 0.12032509,\n",
      "       0.11196733, 0.22237182, 0.13110852, 0.16774368, 0.1136651 ]), 'test_mcc': array([ 0.58528136,  0.39550372,  0.62388446,  0.50410382,  0.55231899,\n",
      "        0.47938458,  0.02701609, -0.00219989,  0.02863809,  0.65537722])}\n",
      "Test score of LGBM: 0.998139110284049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56880\n",
      "           1       0.00      0.00      0.00        82\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.50      0.50      0.50     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm.Dataset(X, categorical_feature='Class')\n",
    "lgb_clf=lgbm.LGBMClassifier()\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(lgb_clf,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = lgb_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of LBGM: {cv_score}\\nTest score of LGBM: {test_score}')\n",
    "print(classification_report(y_test, lgb_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7762fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0007795278740719617\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, lgb_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41886126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of LGBM (SMOTE): {'fit_time': array([18.59146953, 58.15832853, 57.13276482, 42.53495884, 33.17841864,\n",
      "       39.29083371, 20.72060108, 29.89983726, 10.31654859, 30.18591785]), 'score_time': array([0.12132287, 0.14397335, 0.11859822, 0.15015769, 0.15392494,\n",
      "       0.12870455, 0.1121521 , 0.14508939, 0.11900711, 0.13105845]), 'test_mcc': array([0.21392458, 0.23849985, 0.25651347, 0.27554135, 0.27903172,\n",
      "       0.24181979, 0.24534413, 0.21439205, 0.25336139, 0.23824027])}\n",
      "Test score of LGBM (SMOTE): 0.9823039921351077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     56880\n",
      "           1       0.06      0.84      0.12        82\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.53      0.91      0.56     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm.Dataset(X, categorical_feature='Class')\n",
    "lgb_smote_pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=424)],\n",
    "                                        ['lgbm', lgbm.LGBMClassifier()]])\n",
    "\n",
    "\n",
    "lgb_smote_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(lgb_smote_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = lgb_smote_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of LGBM (SMOTE): {cv_score}\\nTest score of LGBM (SMOTE): {test_score}')\n",
    "print(classification_report(y_test, lgb_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9ba6031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23074346325137812\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, lgb_smote_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e758ccc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of LGBM (ADASYN): {'fit_time': array([ 86.04067087,  89.73316503, 115.75333571,  83.64850187,\n",
      "        50.16501117,  79.54669213, 129.70816612, 168.05139399,\n",
      "        58.60698009,  96.12190437]), 'score_time': array([0.39894032, 0.33370042, 0.47789097, 0.35899878, 0.29391408,\n",
      "       0.24134994, 0.31682968, 0.47424769, 0.2395575 , 0.28733015]), 'test_mcc': array([0.11144678, 0.09995681, 0.13850835, 0.13983047, 0.12024008,\n",
      "       0.12500278, 0.11253773, 0.12174544, 0.12298131, 0.12647794])}\n",
      "Test score of LGBM (ADASYN): 0.9156981847547487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     56880\n",
      "           1       0.01      0.85      0.03        82\n",
      "\n",
      "    accuracy                           0.92     56962\n",
      "   macro avg       0.51      0.88      0.49     56962\n",
      "weighted avg       1.00      0.92      0.95     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm.Dataset(X, categorical_feature='Class')\n",
    "lgb_adasyn_pipeline = imbpipeline(steps = [['adasyn', ADASYN(random_state=424)],\n",
    "                                        ['lgbm', lgbm.LGBMClassifier()]])\n",
    "\n",
    "\n",
    "lgb_adasyn_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(lgb_adasyn_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = lgb_adasyn_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of LGBM (ADASYN): {cv_score}\\nTest score of LGBM (ADASYN): {test_score}')\n",
    "print(classification_report(y_test, lgb_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c14e773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10442864058489247\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, lgb_adasyn_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c57b3cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\n",
      "Cross-validation score of LGBM (Nearmiss): {'fit_time': array([33.7268734 , 29.32821918, 32.68744993, 20.02666783, 39.55700994,\n",
      "       19.5903964 , 28.46383858, 30.65648127, 30.35178638,  2.73490977]), 'score_time': array([0.23391485, 0.31941414, 0.21056867, 0.28218126, 0.23108912,\n",
      "       0.08901358, 0.070961  , 0.16439986, 0.09849858, 0.12218714]), 'test_mcc': array([0.00552199, 0.00637141, 0.00792442, 0.00704193, 0.00490286,\n",
      "       0.0104614 , 0.00907932, 0.00865022, 0.00935566, 0.00728043])}\n",
      "Test score of LGBM (Nearmiss): 0.04680313191250307\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.05      0.09     56880\n",
      "           1       0.00      1.00      0.00        82\n",
      "\n",
      "    accuracy                           0.05     56962\n",
      "   macro avg       0.50      0.52      0.04     56962\n",
      "weighted avg       1.00      0.05      0.09     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm.Dataset(X, categorical_feature='Class')\n",
    "lgb_nearmiss_pipeline = imbpipeline(steps = [['nearmiss', NearMiss()],\n",
    "                                        ['lgbm', lgbm.LGBMClassifier()]])\n",
    "\n",
    "\n",
    "lgb_nearmiss_pipeline.fit(X_train, y_train)\n",
    "mcc = {'mcc': make_scorer(matthews_corrcoef)}\n",
    "cv_score = cross_validate(lgb_nearmiss_pipeline,  X_train, y_train, cv = stratified_kfold, scoring = mcc)\n",
    "# cv_score = cross_val_score(pipeline, X_train, y_train, cv = stratified_kfold)\n",
    "test_score = lgb_nearmiss_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Due to computational limitation, this model will not be tuned. The classification report shows the test set result.\")\n",
    "print(f'Cross-validation score of LGBM (Nearmiss): {cv_score}\\nTest score of LGBM (Nearmiss): {test_score}')\n",
    "print(classification_report(y_test, lgb_nearmiss_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accb9891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008276786864053576\n"
     ]
    }
   ],
   "source": [
    "print(matthews_corrcoef(y_test, lgb_nearmiss_pipeline.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
